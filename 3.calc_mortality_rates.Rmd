---
title: "Calculate Mortality Rates"
author: "Aarsh"
date: '2022-10-06'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Global parameters and libraries
```{r lib_glob_parameters}

# libraries
library(readr)
library(dplyr)
library(stringr)
library(magrittr)
library(ggplot2)
library(readr)
library(tidytext)
library(tidyr)
library(tidyverse)
library(sf)
library(usethis)
library(devtools)
library(readxl)


# load and document (for automatic rendering of the roxygen comments into a man file) to keep things updated
devtools::load_all()
devtools::document()

# custom operators
`%notin%` <- Negate(`%in%`)

# global parameters 
gbd_year <- 2019

# global average PM2.5 value (taken from the latest AQLI color dataset that corresponds to the June 2022 AQLI Annual report)
global_avg_pm2.5 <- 27.5

```


## Merge Relative Risks and Mortality Rates

```{r mergeRelativeRisksAndMortalityRates}

#> Read in the cleaned GBD mortality rates file (note that this does not contain the "All causes" data, that is separately downloaded below)
cleaned_gbd_mortality_rates <- read_csv("./data/intermediate/cleaned_gbd_mortality_rates.csv")

#> Read in the cleaned relative risks file
cleaned_relative_risks <- read_csv("./data/intermediate/cleaned_relative_risks.csv")

#> Read in "All causes" cleaned GBD mortality rates data
cleaned_gbd_mortality_rates_all_causes_only <- read_csv("./data/intermediate/cleaned_gbd_mortality_rates_all_causes_only.csv")

#> join the cleaned mortality and relative risks datasets
cleaned_joined_mort_rr_data <- cleaned_gbd_mortality_rates %>%
  left_join(cleaned_relative_risks, by = c("cause", "age_interval"))

#> dropping redundant columns and renaming certain columns after dropping the redundancies, then reordering the columns. Note that, it is possible that some of the function names for dplyr and plyr might clash. So, use the package prefix, before the function name. For example, to rename a column, use: dplyr::rename(), instead of just rename()
cleaned_joined_mort_rr_data <- cleaned_joined_mort_rr_data %>%
 dplyr::select(-c(age_interval_ll.y, age_interval_ul.y, cause_id.y)) %>%
  dplyr::rename(age_interval_ll = age_interval_ll.x, 
         age_interval_ul = age_interval_ul.x, 
         cause_id = cause_id.x) %>%
  dplyr::arrange(cause, age_interval)

#> converting the above dataset back into long format, such that "pm_level" and "relative risk" are columns in the new dataset and sort the dataset by cause, pm_level, age_interval
cleaned_joined_mort_rr_data_long <- cleaned_joined_mort_rr_data %>%
  pivot_longer(cols = relative_risk_pm0:relative_risk_pm90, names_to = c("pm_level"), names_pattern = "(\\d+)", values_to = "relative_risk") %>%
  arrange(cause, pm_level, age_interval)

#> For all age groups less than 25 years of age, relative risks information is not availbale. For these, assume that relative risks = 1. Also replace any missing relative risks information with 1. Below code, replaces all those rows in the "relative_risk" column, where age_interval_ul < 25, with 1. Also, if relative risks data is missing, fill it in with relative risks = 1. 

cleaned_joined_mort_rr_data_long <- cleaned_joined_mort_rr_data_long %>%
  mutate(relative_risk = ifelse(age_interval_ul < 25, 1, relative_risk), 
         relative_risk = ifelse(is.na(relative_risk) == TRUE, 1, relative_risk))

#> coercing certain numeric columns into class "numeric"
cleaned_joined_mort_rr_data_long$pm_level <- as.numeric(cleaned_joined_mort_rr_data_long$pm_level)

#> Find the pm_level in the "cleaned_joined_mort_rr_data_long" dataset that is closest to "global_average_pm2.5" that is set up top in "lib_glob_parameters" chunk.

#> pm2.5 buckets, one of which will contain the global_avg_pm2.5. This bucket will be the one that is closest to the global average PM2.5 value that is set up top. In old STATA scripts, you will find that this process is carried out by a "rounding process". But, that has to be adjusted every year the dataset changes. What I have done below is agnostic to any data changes.

# figure out the unique PM2.5 values in the data and sort them in ascending order
unique_list_of_pm2.5_val_in_data <- sort(unique(cleaned_joined_mort_rr_data_long$pm_level))

# calculate how far the global average pm2.5 value is from each of the values in the above list of unique pm2.5 values
distance_from_global_avg_pm2.5 <- abs(unique_list_of_pm2.5_val_in_data - global_avg_pm2.5)

# *New assumption in here*: Which pm_level value in our current dataset is the closest to the "global_average_pm2.5"? If there are 2 such values, choose the one with a lower pollution number, so that our estimate is a conservation one (for now)
potential_pm2.5_buckets_indices <- which(distance_from_global_avg_pm2.5 == min(distance_from_global_avg_pm2.5))

# if there are more than one potential buckets in which the global_average_pm2.5 can land, this conditional statement below chooses the bucket with a lower pm2.5 level. This is a conservative step that we take for now and will reevaluate its implications.
if(length(potential_pm2.5_buckets_indices) < 2){
  global_avg_pm2.5_rounded <- unique_list_of_pm2.5_val_in_data[potential_pm2.5_buckets_indices]
} else {
  global_avg_pm2.5_rounded <- min(unique_list_of_pm2.5_val_in_data[potential_pm2.5_buckets_indices])
}

#> create a new temp columm that will be used to create yet another "rr_normalizer" column which will be used to create yet another "adjusted_mortality_rates" column. After generating the "rr_normalizer" column, the "temp" column will be dropped as it would have served its purpose by then. The "temp" column in the step below takes in the relative risk number of those rows where pm_level = "global_avg_pm2.5_rounded" number. In other words, it takes on those relative risk numbers that correspond to the current global average pm2.5 concentration level.  
```

