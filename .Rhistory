#> dropping redundant columns and renaming certain columns after dropping the redundancies, then reordering the columns. Note that, it is possible that some of the function names for dplyr and plyr might clash. So, use the package prefix, before the function name. For example, to rename a column, use: dplyr::rename(), instead of just rename()
cleaned_joined_mort_rr_data <- cleaned_joined_mort_rr_data %>%
dplyr::select(-c(age_interval_ul.y, cause_id.y)) %>%
dplyr::rename(age_interval_ul = age_interval_ul.x,
cause_id = cause_id.x) %>%
dplyr::arrange(cause, age_interval_ll)
#> converting the above dataset back into long format, such that "pm_level" and "relative risk" are columns in the new dataset and sort the dataset by cause, pm_level, age_interval
cleaned_joined_mort_rr_data_long <- cleaned_joined_mort_rr_data %>%
pivot_longer(cols = relative_risk_pm0:relative_risk_pm90, names_to = c("pm_level"), names_pattern = "(\\d+)", values_to = "relative_risk") %>%
arrange(cause, age_interval_ll, pm_level)
#> For all age groups less than 25 years of age, relative risks information is not availbale. For these, assume that relative risks = 1. Also replace any missing relative risks information with 1. Below code, replaces all those rows in the "relative_risk" column, where age_interval_ul < 25, with 1. Also, if relative risks data is missing, fill it in with relative risks = 1.
cleaned_joined_mort_rr_data_long <- cleaned_joined_mort_rr_data_long %>%
mutate(relative_risk = ifelse(age_interval_ul < 25, 1, relative_risk),
relative_risk = ifelse(is.na(relative_risk) == TRUE, 1, relative_risk))
#> coercing certain numeric columns into class "numeric"
cleaned_joined_mort_rr_data_long$pm_level <- as.numeric(cleaned_joined_mort_rr_data_long$pm_level)
#> Find the pm_level in the "cleaned_joined_mort_rr_data_long" dataset that is closest to "global_average_pm2.5" that is set up top in "lib_glob_parameters" chunk.
#> pm2.5 buckets, one of which will contain the global_avg_pm2.5. This bucket will be the one that is closest to the global average PM2.5 value that is set up top. In old STATA scripts, you will find that this process is carried out by a "rounding process". But, that has to be adjusted every year the dataset changes. What I have done below is agnostic to any data changes.
# figure out the unique PM2.5 values in the data and sort them in ascending order
unique_list_of_pm2.5_val_in_data <- sort(unique(cleaned_joined_mort_rr_data_long$pm_level))
# calculate how far the global average pm2.5 value is from each of the values in the above list of unique pm2.5 values
distance_from_global_avg_pm2.5 <- abs(unique_list_of_pm2.5_val_in_data - global_avg_pm2.5)
# *New assumption in here*: Which pm_level value in our current dataset is the closest to the "global_average_pm2.5"? If there are 2 such values, choose the one with a lower pollution number, so that our estimate is a conservation one (for now)
potential_pm2.5_buckets_indices <- which(distance_from_global_avg_pm2.5 == min(distance_from_global_avg_pm2.5))
# if there are more than one potential buckets in which the global_average_pm2.5 can land, this conditional statement below chooses the bucket with a lower pm2.5 level. This is a conservative step that we take for now and will reevaluate its implications.
if(length(potential_pm2.5_buckets_indices) < 2){
global_avg_pm2.5_rounded <- unique_list_of_pm2.5_val_in_data[potential_pm2.5_buckets_indices]
} else {
global_avg_pm2.5_rounded <- min(unique_list_of_pm2.5_val_in_data[potential_pm2.5_buckets_indices])
}
#> create a new temp columm that will be used to create yet another "rr_normalizer" column which will be used to create yet another "adjusted_mortality_rates" column. After generating the "rr_normalizer" column, the "temp" column will be dropped as it would have served its purpose by then. The "temp" column in the step below takes in the relative risk number of those rows where pm_level = "global_avg_pm2.5_rounded" number. In other words, it takes on those relative risk numbers that correspond to the current global average pm2.5 concentration level.
# create a new "temp" column
cleaned_joined_mort_rr_data_long <- cleaned_joined_mort_rr_data_long %>%
mutate(temp = ifelse(pm_level == global_avg_pm2.5_rounded, relative_risk, NA))
# group by cause, and age_interval and each row of each of these groups, gets the relative risk number that is equal to mean of "temp" column for that group. Capture this new information in a new column called "rr_normalizer".
cleaned_joined_mort_rr_data_long <- cleaned_joined_mort_rr_data_long %>%
group_by(cause, age_interval_ll) %>%
mutate(rr_normalizer = mean(temp, na.rm = TRUE)) %>%
ungroup()
# drop the temp column
cleaned_joined_mort_rr_data_long <- cleaned_joined_mort_rr_data_long %>%
select(-temp)
# add a new column called "adjusted mortality rate", which adjusts the mortality rate for a given age_cat, cause group, by their RR in relation to the RR at the rounded Global Average PM2.5. Here the assumption is that in the absence of the risk factor, age specific death rates would be proportionally lower. This dataset corresponds to the "file1" dataset in the STATA script. This is what "PM2.5 specific mortality rates" would be if PM2.5 = X. This helps us calcuate mortality rates for any given counterfactual PM2.5 value "X".
cleaned_joined_mort_rr_data_long <- cleaned_joined_mort_rr_data_long %>%
mutate(adjusted_mortality_rate = (mortality_rate * relative_risk)/rr_normalizer)
#>---------------------------------------------------------------------------------------------->#
#>---------------- Prep "all cause" mortality rates (loaded as a separate file up top) for "actual life table computation", adjusted to reflect higher (or lower) mortality rates at PM2.5 concentrations that are higher (or lower) than the global average.------------------------------------------------->#
# create a m_diff column, that takes the difference between "actual" and "adjusted" mortality rates
cleaned_joined_mort_rr_data_long <- cleaned_joined_mort_rr_data_long %>%
mutate(m_diff = adjusted_mortality_rate - mortality_rate)
# * (New Assumption): summarise "cleaned_joined_mort_rr_data_long" dataset by summing up "m_diff", grouped by age_cat, pm_level and then sort by "pm_level" and "age_cat". Note that this m_diff_sum (which is calculated using the adjusted mortality rates concerning the 6 causes of deaths) will be used to adjust all cause mortality rates later on in the process. It should be noted that the "m_diff_sum" that we will add to the "all cause mortality" later on in the process, is an adjustment based on the 6 disease channels in question.
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long %>%
group_by(age_interval_ll, pm_level) %>%
summarise(m_diff_sum = sum(m_diff, na.rm = TRUE)) %>%
ungroup() %>%
arrange(pm_level, age_interval_ll)
# (* New Assumption) merging the summary dataset generated in the above step with "cleaned_gbd_mortality_rates_all_causes" dataset, using "age_interval" as the linking key. Then rename the "mortality_rate" column to "actual_mr". Note that the "all_causes" dataset does not have a "pm_level" column. So, each age_category in the joined dataset, has an associated set of 21 pm_levels, but for all of those the mortality_rate remains the same (that is an assumption that we make). We then adjust these mortality rates, by adding in the "m_diff_sum" column to the "actual_mr" column (which is the new name of the "mortality_rate" column), the output of which is assigned to the column named "nMx".
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long_summary %>%
left_join(cleaned_gbd_mortality_rates_all_causes_only, by = "age_interval_ll") %>%
arrange(age_interval_ll, pm_level) %>%
rename(actual_mr = mortality_rate)
# add a new nMx column, which is created by adding the "m_diff_sum" adjustment to the all cause mortality rates. But, note that "m_diff_sum" was calculated from only 6 PM2.5 specific disease channels. This is what "all cause mortality" rates would be if PM2.5 = X (where "X" is a counterfactual concentration)
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long_summary %>%
mutate(nmx = actual_mr + m_diff_sum)
# replace nMx with nmx/100000 (this column represents the "Mortality rate (deaths per person-year), adjusted"): This dataset corresponds to the "file2" dataset in Ken's STATA script. nMx is the mortality incidence rate for age interval between ages x and x + n, expressed in the units of deaths per person-year
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long_summary %>%
mutate(nmx = nmx/100000)
#>----------------------------------------------------------------------------------------------------------->#
#> Prepare age-specific mortalities attributed to PM2.5 (for cause-deleted)------------------------------------------>#
# Note that "cleaned_joined_mort_rr_data_long" corresponds to the "file1" tempfile in the STATA scripts. Adding 2 new columns called paf (Population Attributable Fraction) and pm_mortal_rate
cleaned_joined_mort_rr_data_long <- cleaned_joined_mort_rr_data_long %>%
mutate(paf = 1 - (1/relative_risk),
pm_mortality_rate = paf * adjusted_mortality_rate)
# summary for cause-deleted life table (cdlt): This is the sum of total number of deaths across all 6 causes of death (corresponds to collapse statement that we see in part "f" of the STATA script).
cleaned_joined_mort_rr_data_long_summary_cdlt <- cleaned_joined_mort_rr_data_long %>%
group_by(age_interval, age_interval_ll, age_interval_ul, age_gap, pm_level) %>%
summarise(pm_mortality_rate = sum(pm_mortality_rate, na.rm = TRUE)) %>%
ungroup()
# rename pm_mortality_rate to pm_nmx and then converting pm_nmx into a rate by dividing by 100000. The resulting dataset corresponds to "file 3" in the STATA script
cleaned_joined_mort_rr_data_long_summary_cdlt <- cleaned_joined_mort_rr_data_long_summary_cdlt %>%
rename(pm_nmx = pm_mortality_rate) %>%
mutate(pm_nmx = pm_nmx/100000)
#>--------------------------------------------------------------------------------------------------------------------------->#
#> Calcualte "Actual" Global Life Table, for various PM2.5 levels (corresponding to start of Part-2 in the STATA script)--------------->#
# Use "adjusted" all cause mortality rates, to compute actual life expectancy at birth in a world where PM2.5 = x (following a combination of Apte's 2018 paper supplemental information doc and Arias 2013 paper, which follow same terminology).
# define alpha_x to be 0.5 for every age group (as in Apte et al.). alpha represents the fraction of the age interval duration that the average dying cohort member survives. Given this alpha_x = 0.5, tells us that we assume deaths take place at the midpoint of each age interval. Note that the dataset we use below is the same one that we prepped above, i.e. "cleaned_joined_mort_rr_data_long_summary", i.e. file 2 in the corresponding STATA script.
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long_summary %>%
mutate(alpha_x = 0.5)
# compute nqx (its the probability of death during the age interval of duration n years i.e.between age x and age x+n)
# nqx = ndx/lx (equation 3, Arias, et al, 2013 & also Eq.3 of Apte's 2018 supplemental information doc). ndx represents the number of life table cohort deaths in a given age interval (x, x+n), from a given cause.
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long_summary %>%
mutate(nqx = (nmx*age_gap)/(1 + ((1 - alpha_x)*(nmx)*(age_gap))))
# (* Assumption): set nqx = 1, if age_cat = max(age_cat), which as of now is "95+" and sorting by pm_level and age_interval
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long_summary %>%
mutate(nqx = ifelse(age_interval == "95+", 1, nqx)) %>%
arrange(pm_level, age_interval_ll)
#> Compute lx (Population of cohort still alive at the beginning of the age interval).
# lx is the survivor function. It considers the surviving population of a hypothetical birth cohort of 100000 individuals at age x. See equation 2 in Apte et al, 2018 (supplemental information doc). In other words, population of the lifetable cohort that is still alive at the beginning of age interval x depends on the number of individuals that are alive at the outset of the previous age interval x-l, and the fraction of members who survived that preceding age interval, (1 - nq(x-1)). For the first age interval (0-1), lx = 100000, because the everyone is alive at the beginning of the first age interval. For every interval, after the first one, we use the formula in equation 3 of the apte paper to calculate lx.
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long_summary %>%
mutate(lx = 100000) %>%
group_by(pm_level) %>%
mutate(temp_grp_wise_row_number = dplyr::row_number()) %>%  # adding this column so that we can avoid writing a loop to calculate lx in the next mutate statement
mutate(lx = ifelse(age_interval != "0-1", (lx[temp_grp_wise_row_number -1] * (1 - nqx[temp_grp_wise_row_number - 1])), 100000)) %>%
ungroup()
#> Compute ndx i.e. the life table cohort deaths in the given  age interval and nlx, i.e. life years lived by the cohort in the given age interval
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long_summary %>%
mutate(ndx = lx * nqx,
nlx = ndx/nmx)
#> Compute nfx (which is a preparation for calculating the global "cause deleted" life table, that will be calculated after we are done with the "actual life table" calculation.
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long_summary %>%
arrange(pm_level, age_interval_ll) %>%
mutate(nfx = 0) %>%
group_by(pm_level) %>%
mutate(nfx = (((age_gap[temp_grp_wise_row_number])*(lx[temp_grp_wise_row_number])) - nlx[temp_grp_wise_row_number])/(lx[temp_grp_wise_row_number] - lx[temp_grp_wise_row_number + 1])) %>%
ungroup()
#> compute ntx
# for age_interval_category == max(age_interval_category), set ntx = nlx
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long_summary %>%
mutate(ntx = NA,
ntx = ifelse(age_interval_category == max(age_interval_category), nlx, ntx))
# for the rest, for each pm_level (group by pm_level) -> update ntx, by starting from max(age_interval_category) - 1 (which in our case will be the 90-94 age interval) and then, go back till the first interval. This means that for a given pm_level group, we would start at the bottom of the group and then go up.
# number of unique age catgories
num_unique_age_cat <- max(cleaned_joined_mort_rr_data_long_summary$age_interval_category)
# number of unique pm levels (= nrow(cleaned_joined_mort_rr_data_long_summary)/num_unique_age_cat)
num_unique_pm_levels <- length(unique(cleaned_joined_mort_rr_data_long_summary$pm_level))
#
for(i in 1:num_unique_pm_levels){
if(i == 1){
temp_grp <- cleaned_joined_mort_rr_data_long_summary[1 : num_unique_age_cat, ]
} else {
temp_grp <- cleaned_joined_mort_rr_data_long_summary[((((i - 1)*(num_unique_age_cat)) + 1) :(i * num_unique_age_cat)), ]
}
print(str_c("Iteration number", i, sep = "-"))
for(j in (num_unique_age_cat - 1): 1){
temp_grp <- temp_grp %>%
select(ntx, nlx) %>%
mutate(ntx = nlx[j] + ntx[j + 1])
}
if(i == 1){
cleaned_joined_mort_rr_data_long_summary$ntx[1:num_unique_age_cat] <- temp_grp$ntx
} else {
cleaned_joined_mort_rr_data_long_summary$ntx[((((i - 1)*(num_unique_age_cat)) + 1) :(i * num_unique_age_cat))] <- temp_grp$ntx
}
}
#> Read in the cleaned GBD mortality rates file (note that this does not contain the "All causes" data, that is separately downloaded below)
cleaned_gbd_mortality_rates <- read_csv("./data/intermediate/cleaned_gbd_mortality_rates.csv")
#> Read in the cleaned relative risks file
cleaned_relative_risks <- read_csv("./data/intermediate/cleaned_relative_risks.csv")
#> Read in "All causes" cleaned GBD mortality rates data
cleaned_gbd_mortality_rates_all_causes_only <- read_csv("./data/intermediate/cleaned_gbd_mortality_rates_all_causes_only.csv")
#> join the cleaned mortality and relative risks datasets
cleaned_joined_mort_rr_data <- cleaned_gbd_mortality_rates %>%
left_join(cleaned_relative_risks, by = c("cause", "age_interval", "age_interval_ll"))
#> dropping redundant columns and renaming certain columns after dropping the redundancies, then reordering the columns. Note that, it is possible that some of the function names for dplyr and plyr might clash. So, use the package prefix, before the function name. For example, to rename a column, use: dplyr::rename(), instead of just rename()
cleaned_joined_mort_rr_data <- cleaned_joined_mort_rr_data %>%
dplyr::select(-c(age_interval_ul.y, cause_id.y)) %>%
dplyr::rename(age_interval_ul = age_interval_ul.x,
cause_id = cause_id.x) %>%
dplyr::arrange(cause, age_interval_ll)
#> converting the above dataset back into long format, such that "pm_level" and "relative risk" are columns in the new dataset and sort the dataset by cause, pm_level, age_interval
cleaned_joined_mort_rr_data_long <- cleaned_joined_mort_rr_data %>%
pivot_longer(cols = relative_risk_pm0:relative_risk_pm90, names_to = c("pm_level"), names_pattern = "(\\d+)", values_to = "relative_risk") %>%
arrange(cause, age_interval_ll, pm_level)
#> For all age groups less than 25 years of age, relative risks information is not availbale. For these, assume that relative risks = 1. Also replace any missing relative risks information with 1. Below code, replaces all those rows in the "relative_risk" column, where age_interval_ul < 25, with 1. Also, if relative risks data is missing, fill it in with relative risks = 1.
cleaned_joined_mort_rr_data_long <- cleaned_joined_mort_rr_data_long %>%
mutate(relative_risk = ifelse(age_interval_ul < 25, 1, relative_risk),
relative_risk = ifelse(is.na(relative_risk) == TRUE, 1, relative_risk))
#> coercing certain numeric columns into class "numeric"
cleaned_joined_mort_rr_data_long$pm_level <- as.numeric(cleaned_joined_mort_rr_data_long$pm_level)
#> Find the pm_level in the "cleaned_joined_mort_rr_data_long" dataset that is closest to "global_average_pm2.5" that is set up top in "lib_glob_parameters" chunk.
#> pm2.5 buckets, one of which will contain the global_avg_pm2.5. This bucket will be the one that is closest to the global average PM2.5 value that is set up top. In old STATA scripts, you will find that this process is carried out by a "rounding process". But, that has to be adjusted every year the dataset changes. What I have done below is agnostic to any data changes.
# figure out the unique PM2.5 values in the data and sort them in ascending order
unique_list_of_pm2.5_val_in_data <- sort(unique(cleaned_joined_mort_rr_data_long$pm_level))
# calculate how far the global average pm2.5 value is from each of the values in the above list of unique pm2.5 values
distance_from_global_avg_pm2.5 <- abs(unique_list_of_pm2.5_val_in_data - global_avg_pm2.5)
# *New assumption in here*: Which pm_level value in our current dataset is the closest to the "global_average_pm2.5"? If there are 2 such values, choose the one with a lower pollution number, so that our estimate is a conservation one (for now)
potential_pm2.5_buckets_indices <- which(distance_from_global_avg_pm2.5 == min(distance_from_global_avg_pm2.5))
# if there are more than one potential buckets in which the global_average_pm2.5 can land, this conditional statement below chooses the bucket with a lower pm2.5 level. This is a conservative step that we take for now and will reevaluate its implications.
if(length(potential_pm2.5_buckets_indices) < 2){
global_avg_pm2.5_rounded <- unique_list_of_pm2.5_val_in_data[potential_pm2.5_buckets_indices]
} else {
global_avg_pm2.5_rounded <- min(unique_list_of_pm2.5_val_in_data[potential_pm2.5_buckets_indices])
}
#> create a new temp columm that will be used to create yet another "rr_normalizer" column which will be used to create yet another "adjusted_mortality_rates" column. After generating the "rr_normalizer" column, the "temp" column will be dropped as it would have served its purpose by then. The "temp" column in the step below takes in the relative risk number of those rows where pm_level = "global_avg_pm2.5_rounded" number. In other words, it takes on those relative risk numbers that correspond to the current global average pm2.5 concentration level.
# create a new "temp" column
cleaned_joined_mort_rr_data_long <- cleaned_joined_mort_rr_data_long %>%
mutate(temp = ifelse(pm_level == global_avg_pm2.5_rounded, relative_risk, NA))
# group by cause, and age_interval and each row of each of these groups, gets the relative risk number that is equal to mean of "temp" column for that group. Capture this new information in a new column called "rr_normalizer".
cleaned_joined_mort_rr_data_long <- cleaned_joined_mort_rr_data_long %>%
group_by(cause, age_interval_ll) %>%
mutate(rr_normalizer = mean(temp, na.rm = TRUE)) %>%
ungroup()
# drop the temp column
cleaned_joined_mort_rr_data_long <- cleaned_joined_mort_rr_data_long %>%
select(-temp)
# add a new column called "adjusted mortality rate", which adjusts the mortality rate for a given age_cat, cause group, by their RR in relation to the RR at the rounded Global Average PM2.5. Here the assumption is that in the absence of the risk factor, age specific death rates would be proportionally lower. This dataset corresponds to the "file1" dataset in the STATA script. This is what "PM2.5 specific mortality rates" would be if PM2.5 = X. This helps us calcuate mortality rates for any given counterfactual PM2.5 value "X".
cleaned_joined_mort_rr_data_long <- cleaned_joined_mort_rr_data_long %>%
mutate(adjusted_mortality_rate = (mortality_rate * relative_risk)/rr_normalizer)
#>---------------------------------------------------------------------------------------------->#
#>---------------- Prep "all cause" mortality rates (loaded as a separate file up top) for "actual life table computation", adjusted to reflect higher (or lower) mortality rates at PM2.5 concentrations that are higher (or lower) than the global average.------------------------------------------------->#
# create a m_diff column, that takes the difference between "actual" and "adjusted" mortality rates
cleaned_joined_mort_rr_data_long <- cleaned_joined_mort_rr_data_long %>%
mutate(m_diff = adjusted_mortality_rate - mortality_rate)
# * (New Assumption): summarise "cleaned_joined_mort_rr_data_long" dataset by summing up "m_diff", grouped by age_cat, pm_level and then sort by "pm_level" and "age_cat". Note that this m_diff_sum (which is calculated using the adjusted mortality rates concerning the 6 causes of deaths) will be used to adjust all cause mortality rates later on in the process. It should be noted that the "m_diff_sum" that we will add to the "all cause mortality" later on in the process, is an adjustment based on the 6 disease channels in question.
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long %>%
group_by(age_interval_ll, pm_level) %>%
summarise(m_diff_sum = sum(m_diff, na.rm = TRUE)) %>%
ungroup() %>%
arrange(pm_level, age_interval_ll)
# (* New Assumption) merging the summary dataset generated in the above step with "cleaned_gbd_mortality_rates_all_causes" dataset, using "age_interval" as the linking key. Then rename the "mortality_rate" column to "actual_mr". Note that the "all_causes" dataset does not have a "pm_level" column. So, each age_category in the joined dataset, has an associated set of 21 pm_levels, but for all of those the mortality_rate remains the same (that is an assumption that we make). We then adjust these mortality rates, by adding in the "m_diff_sum" column to the "actual_mr" column (which is the new name of the "mortality_rate" column), the output of which is assigned to the column named "nMx".
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long_summary %>%
left_join(cleaned_gbd_mortality_rates_all_causes_only, by = "age_interval_ll") %>%
arrange(age_interval_ll, pm_level) %>%
rename(actual_mr = mortality_rate)
# add a new nMx column, which is created by adding the "m_diff_sum" adjustment to the all cause mortality rates. But, note that "m_diff_sum" was calculated from only 6 PM2.5 specific disease channels. This is what "all cause mortality" rates would be if PM2.5 = X (where "X" is a counterfactual concentration)
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long_summary %>%
mutate(nmx = actual_mr + m_diff_sum)
# replace nMx with nmx/100000 (this column represents the "Mortality rate (deaths per person-year), adjusted"): This dataset corresponds to the "file2" dataset in Ken's STATA script. nMx is the mortality incidence rate for age interval between ages x and x + n, expressed in the units of deaths per person-year
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long_summary %>%
mutate(nmx = nmx/100000)
#>----------------------------------------------------------------------------------------------------------->#
#> Prepare age-specific mortalities attributed to PM2.5 (for cause-deleted)------------------------------------------>#
# Note that "cleaned_joined_mort_rr_data_long" corresponds to the "file1" tempfile in the STATA scripts. Adding 2 new columns called paf (Population Attributable Fraction) and pm_mortal_rate
cleaned_joined_mort_rr_data_long <- cleaned_joined_mort_rr_data_long %>%
mutate(paf = 1 - (1/relative_risk),
pm_mortality_rate = paf * adjusted_mortality_rate)
# summary for cause-deleted life table (cdlt): This is the sum of total number of deaths across all 6 causes of death (corresponds to collapse statement that we see in part "f" of the STATA script).
cleaned_joined_mort_rr_data_long_summary_cdlt <- cleaned_joined_mort_rr_data_long %>%
group_by(age_interval, age_interval_ll, age_interval_ul, age_gap, pm_level) %>%
summarise(pm_mortality_rate = sum(pm_mortality_rate, na.rm = TRUE)) %>%
ungroup()
# rename pm_mortality_rate to pm_nmx and then converting pm_nmx into a rate by dividing by 100000. The resulting dataset corresponds to "file 3" in the STATA script
cleaned_joined_mort_rr_data_long_summary_cdlt <- cleaned_joined_mort_rr_data_long_summary_cdlt %>%
rename(pm_nmx = pm_mortality_rate) %>%
mutate(pm_nmx = pm_nmx/100000)
#>--------------------------------------------------------------------------------------------------------------------------->#
#> Calcualte "Actual" Global Life Table, for various PM2.5 levels (corresponding to start of Part-2 in the STATA script)--------------->#
# Use "adjusted" all cause mortality rates, to compute actual life expectancy at birth in a world where PM2.5 = x (following a combination of Apte's 2018 paper supplemental information doc and Arias 2013 paper, which follow same terminology).
# define alpha_x to be 0.5 for every age group (as in Apte et al.). alpha represents the fraction of the age interval duration that the average dying cohort member survives. Given this alpha_x = 0.5, tells us that we assume deaths take place at the midpoint of each age interval. Note that the dataset we use below is the same one that we prepped above, i.e. "cleaned_joined_mort_rr_data_long_summary", i.e. file 2 in the corresponding STATA script.
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long_summary %>%
mutate(alpha_x = 0.5)
# compute nqx (its the probability of death during the age interval of duration n years i.e.between age x and age x+n)
# nqx = ndx/lx (equation 3, Arias, et al, 2013 & also Eq.3 of Apte's 2018 supplemental information doc). ndx represents the number of life table cohort deaths in a given age interval (x, x+n), from a given cause.
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long_summary %>%
mutate(nqx = (nmx*age_gap)/(1 + ((1 - alpha_x)*(nmx)*(age_gap))))
# (* Assumption): set nqx = 1, if age_cat = max(age_cat), which as of now is "95+" and sorting by pm_level and age_interval
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long_summary %>%
mutate(nqx = ifelse(age_interval == "95+", 1, nqx)) %>%
arrange(pm_level, age_interval_ll)
#> Compute lx (Population of cohort still alive at the beginning of the age interval).
# lx is the survivor function. It considers the surviving population of a hypothetical birth cohort of 100000 individuals at age x. See equation 2 in Apte et al, 2018 (supplemental information doc). In other words, population of the lifetable cohort that is still alive at the beginning of age interval x depends on the number of individuals that are alive at the outset of the previous age interval x-l, and the fraction of members who survived that preceding age interval, (1 - nq(x-1)). For the first age interval (0-1), lx = 100000, because the everyone is alive at the beginning of the first age interval. For every interval, after the first one, we use the formula in equation 3 of the apte paper to calculate lx.
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long_summary %>%
mutate(lx = 100000) %>%
group_by(pm_level) %>%
mutate(temp_grp_wise_row_number = dplyr::row_number()) %>%  # adding this column so that we can avoid writing a loop to calculate lx in the next mutate statement
mutate(lx = ifelse(age_interval != "0-1", (lx[temp_grp_wise_row_number -1] * (1 - nqx[temp_grp_wise_row_number - 1])), 100000)) %>%
ungroup()
#> Compute ndx i.e. the life table cohort deaths in the given  age interval and nlx, i.e. life years lived by the cohort in the given age interval
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long_summary %>%
mutate(ndx = lx * nqx,
nlx = ndx/nmx)
#> Compute nfx (which is a preparation for calculating the global "cause deleted" life table, that will be calculated after we are done with the "actual life table" calculation.
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long_summary %>%
arrange(pm_level, age_interval_ll) %>%
mutate(nfx = 0) %>%
group_by(pm_level) %>%
mutate(nfx = (((age_gap[temp_grp_wise_row_number])*(lx[temp_grp_wise_row_number])) - nlx[temp_grp_wise_row_number])/(lx[temp_grp_wise_row_number] - lx[temp_grp_wise_row_number + 1])) %>%
ungroup()
#> compute ntx
# for age_interval_category == max(age_interval_category), set ntx = nlx
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long_summary %>%
mutate(ntx = NA,
ntx = ifelse(age_interval_category == max(age_interval_category), nlx, ntx))
# for the rest, for each pm_level (group by pm_level) -> update ntx, by starting from max(age_interval_category) - 1 (which in our case will be the 90-94 age interval) and then, go back till the first interval. This means that for a given pm_level group, we would start at the bottom of the group and then go up.
# number of unique age catgories
num_unique_age_cat <- max(cleaned_joined_mort_rr_data_long_summary$age_interval_category)
# number of unique pm levels (= nrow(cleaned_joined_mort_rr_data_long_summary)/num_unique_age_cat)
num_unique_pm_levels <- length(unique(cleaned_joined_mort_rr_data_long_summary$pm_level))
#
for(i in 1:num_unique_pm_levels){
if(i == 1){
temp_grp <- cleaned_joined_mort_rr_data_long_summary[1 : num_unique_age_cat, ]
} else {
temp_grp <- cleaned_joined_mort_rr_data_long_summary[((((i - 1)*(num_unique_age_cat)) + 1) :(i * num_unique_age_cat)), ]
}
temp_grp <- temp_grp %>%
select(ntx, nlx)
print(str_c("Iteration number", i, sep = "-"))
for(j in (num_unique_age_cat - 1): 1){
temp_grp <- temp_grp %>%
mutate(ntx = nlx[j] + ntx[j + 1])
}
if(i == 1){
cleaned_joined_mort_rr_data_long_summary$ntx[1:num_unique_age_cat] <- temp_grp$ntx
} else {
cleaned_joined_mort_rr_data_long_summary$ntx[((((i - 1)*(num_unique_age_cat)) + 1) :(i * num_unique_age_cat))] <- temp_grp$ntx
}
}
knitr::opts_chunk$set(echo = TRUE)
# libraries
library(readr)
library(dplyr)
library(stringr)
library(magrittr)
library(ggplot2)
library(readr)
library(tidytext)
library(tidyr)
library(tidyverse)
library(sf)
library(usethis)
library(devtools)
library(readxl)
# load and document (for automatic rendering of the roxygen comments into a man file) to keep things updated
devtools::load_all()
devtools::document()
# custom operators
`%notin%` <- Negate(`%in%`)
# global parameters
gbd_year <- 2019
# global average PM2.5 value (taken from the latest AQLI color dataset that corresponds to the June 2022 AQLI Annual report)
global_avg_pm2.5 <- 27.5
#> Read in the cleaned GBD mortality rates file (note that this does not contain the "All causes" data, that is separately downloaded below)
cleaned_gbd_mortality_rates <- read_csv("./data/intermediate/cleaned_gbd_mortality_rates.csv")
#> Read in the cleaned relative risks file
cleaned_relative_risks <- read_csv("./data/intermediate/cleaned_relative_risks.csv")
#> Read in "All causes" cleaned GBD mortality rates data
cleaned_gbd_mortality_rates_all_causes_only <- read_csv("./data/intermediate/cleaned_gbd_mortality_rates_all_causes_only.csv")
#> join the cleaned mortality and relative risks datasets
cleaned_joined_mort_rr_data <- cleaned_gbd_mortality_rates %>%
left_join(cleaned_relative_risks, by = c("cause", "age_interval", "age_interval_ll"))
#> dropping redundant columns and renaming certain columns after dropping the redundancies, then reordering the columns. Note that, it is possible that some of the function names for dplyr and plyr might clash. So, use the package prefix, before the function name. For example, to rename a column, use: dplyr::rename(), instead of just rename()
cleaned_joined_mort_rr_data <- cleaned_joined_mort_rr_data %>%
dplyr::select(-c(age_interval_ul.y, cause_id.y)) %>%
dplyr::rename(age_interval_ul = age_interval_ul.x,
cause_id = cause_id.x) %>%
dplyr::arrange(cause, age_interval_ll)
#> converting the above dataset back into long format, such that "pm_level" and "relative risk" are columns in the new dataset and sort the dataset by cause, pm_level, age_interval
cleaned_joined_mort_rr_data_long <- cleaned_joined_mort_rr_data %>%
pivot_longer(cols = relative_risk_pm0:relative_risk_pm90, names_to = c("pm_level"), names_pattern = "(\\d+)", values_to = "relative_risk") %>%
arrange(cause, age_interval_ll, pm_level)
#> For all age groups less than 25 years of age, relative risks information is not availbale. For these, assume that relative risks = 1. Also replace any missing relative risks information with 1. Below code, replaces all those rows in the "relative_risk" column, where age_interval_ul < 25, with 1. Also, if relative risks data is missing, fill it in with relative risks = 1.
cleaned_joined_mort_rr_data_long <- cleaned_joined_mort_rr_data_long %>%
mutate(relative_risk = ifelse(age_interval_ul < 25, 1, relative_risk),
relative_risk = ifelse(is.na(relative_risk) == TRUE, 1, relative_risk))
#> coercing certain numeric columns into class "numeric"
cleaned_joined_mort_rr_data_long$pm_level <- as.numeric(cleaned_joined_mort_rr_data_long$pm_level)
#> Find the pm_level in the "cleaned_joined_mort_rr_data_long" dataset that is closest to "global_average_pm2.5" that is set up top in "lib_glob_parameters" chunk.
#> pm2.5 buckets, one of which will contain the global_avg_pm2.5. This bucket will be the one that is closest to the global average PM2.5 value that is set up top. In old STATA scripts, you will find that this process is carried out by a "rounding process". But, that has to be adjusted every year the dataset changes. What I have done below is agnostic to any data changes.
# figure out the unique PM2.5 values in the data and sort them in ascending order
unique_list_of_pm2.5_val_in_data <- sort(unique(cleaned_joined_mort_rr_data_long$pm_level))
# calculate how far the global average pm2.5 value is from each of the values in the above list of unique pm2.5 values
distance_from_global_avg_pm2.5 <- abs(unique_list_of_pm2.5_val_in_data - global_avg_pm2.5)
# *New assumption in here*: Which pm_level value in our current dataset is the closest to the "global_average_pm2.5"? If there are 2 such values, choose the one with a lower pollution number, so that our estimate is a conservation one (for now)
potential_pm2.5_buckets_indices <- which(distance_from_global_avg_pm2.5 == min(distance_from_global_avg_pm2.5))
# if there are more than one potential buckets in which the global_average_pm2.5 can land, this conditional statement below chooses the bucket with a lower pm2.5 level. This is a conservative step that we take for now and will reevaluate its implications.
if(length(potential_pm2.5_buckets_indices) < 2){
global_avg_pm2.5_rounded <- unique_list_of_pm2.5_val_in_data[potential_pm2.5_buckets_indices]
} else {
global_avg_pm2.5_rounded <- min(unique_list_of_pm2.5_val_in_data[potential_pm2.5_buckets_indices])
}
#> create a new temp columm that will be used to create yet another "rr_normalizer" column which will be used to create yet another "adjusted_mortality_rates" column. After generating the "rr_normalizer" column, the "temp" column will be dropped as it would have served its purpose by then. The "temp" column in the step below takes in the relative risk number of those rows where pm_level = "global_avg_pm2.5_rounded" number. In other words, it takes on those relative risk numbers that correspond to the current global average pm2.5 concentration level.
# create a new "temp" column
cleaned_joined_mort_rr_data_long <- cleaned_joined_mort_rr_data_long %>%
mutate(temp = ifelse(pm_level == global_avg_pm2.5_rounded, relative_risk, NA))
# group by cause, and age_interval and each row of each of these groups, gets the relative risk number that is equal to mean of "temp" column for that group. Capture this new information in a new column called "rr_normalizer".
cleaned_joined_mort_rr_data_long <- cleaned_joined_mort_rr_data_long %>%
group_by(cause, age_interval_ll) %>%
mutate(rr_normalizer = mean(temp, na.rm = TRUE)) %>%
ungroup()
# drop the temp column
cleaned_joined_mort_rr_data_long <- cleaned_joined_mort_rr_data_long %>%
select(-temp)
# add a new column called "adjusted mortality rate", which adjusts the mortality rate for a given age_cat, cause group, by their RR in relation to the RR at the rounded Global Average PM2.5. Here the assumption is that in the absence of the risk factor, age specific death rates would be proportionally lower. This dataset corresponds to the "file1" dataset in the STATA script. This is what "PM2.5 specific mortality rates" would be if PM2.5 = X. This helps us calcuate mortality rates for any given counterfactual PM2.5 value "X".
cleaned_joined_mort_rr_data_long <- cleaned_joined_mort_rr_data_long %>%
mutate(adjusted_mortality_rate = (mortality_rate * relative_risk)/rr_normalizer)
#>---------------------------------------------------------------------------------------------->#
#>---------------- Prep "all cause" mortality rates (loaded as a separate file up top) for "actual life table computation", adjusted to reflect higher (or lower) mortality rates at PM2.5 concentrations that are higher (or lower) than the global average.------------------------------------------------->#
# create a m_diff column, that takes the difference between "actual" and "adjusted" mortality rates
cleaned_joined_mort_rr_data_long <- cleaned_joined_mort_rr_data_long %>%
mutate(m_diff = adjusted_mortality_rate - mortality_rate)
# * (New Assumption): summarise "cleaned_joined_mort_rr_data_long" dataset by summing up "m_diff", grouped by age_cat, pm_level and then sort by "pm_level" and "age_cat". Note that this m_diff_sum (which is calculated using the adjusted mortality rates concerning the 6 causes of deaths) will be used to adjust all cause mortality rates later on in the process. It should be noted that the "m_diff_sum" that we will add to the "all cause mortality" later on in the process, is an adjustment based on the 6 disease channels in question.
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long %>%
group_by(age_interval_ll, pm_level) %>%
summarise(m_diff_sum = sum(m_diff, na.rm = TRUE)) %>%
ungroup() %>%
arrange(pm_level, age_interval_ll)
# (* New Assumption) merging the summary dataset generated in the above step with "cleaned_gbd_mortality_rates_all_causes" dataset, using "age_interval" as the linking key. Then rename the "mortality_rate" column to "actual_mr". Note that the "all_causes" dataset does not have a "pm_level" column. So, each age_category in the joined dataset, has an associated set of 21 pm_levels, but for all of those the mortality_rate remains the same (that is an assumption that we make). We then adjust these mortality rates, by adding in the "m_diff_sum" column to the "actual_mr" column (which is the new name of the "mortality_rate" column), the output of which is assigned to the column named "nMx".
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long_summary %>%
left_join(cleaned_gbd_mortality_rates_all_causes_only, by = "age_interval_ll") %>%
arrange(age_interval_ll, pm_level) %>%
rename(actual_mr = mortality_rate)
# add a new nMx column, which is created by adding the "m_diff_sum" adjustment to the all cause mortality rates. But, note that "m_diff_sum" was calculated from only 6 PM2.5 specific disease channels. This is what "all cause mortality" rates would be if PM2.5 = X (where "X" is a counterfactual concentration)
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long_summary %>%
mutate(nmx = actual_mr + m_diff_sum)
# replace nMx with nmx/100000 (this column represents the "Mortality rate (deaths per person-year), adjusted"): This dataset corresponds to the "file2" dataset in Ken's STATA script. nMx is the mortality incidence rate for age interval between ages x and x + n, expressed in the units of deaths per person-year
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long_summary %>%
mutate(nmx = nmx/100000)
#>----------------------------------------------------------------------------------------------------------->#
#> Prepare age-specific mortalities attributed to PM2.5 (for cause-deleted)------------------------------------------>#
# Note that "cleaned_joined_mort_rr_data_long" corresponds to the "file1" tempfile in the STATA scripts. Adding 2 new columns called paf (Population Attributable Fraction) and pm_mortal_rate
cleaned_joined_mort_rr_data_long <- cleaned_joined_mort_rr_data_long %>%
mutate(paf = 1 - (1/relative_risk),
pm_mortality_rate = paf * adjusted_mortality_rate)
# summary for cause-deleted life table (cdlt): This is the sum of total number of deaths across all 6 causes of death (corresponds to collapse statement that we see in part "f" of the STATA script).
cleaned_joined_mort_rr_data_long_summary_cdlt <- cleaned_joined_mort_rr_data_long %>%
group_by(age_interval, age_interval_ll, age_interval_ul, age_gap, pm_level) %>%
summarise(pm_mortality_rate = sum(pm_mortality_rate, na.rm = TRUE)) %>%
ungroup()
# rename pm_mortality_rate to pm_nmx and then converting pm_nmx into a rate by dividing by 100000. The resulting dataset corresponds to "file 3" in the STATA script
cleaned_joined_mort_rr_data_long_summary_cdlt <- cleaned_joined_mort_rr_data_long_summary_cdlt %>%
rename(pm_nmx = pm_mortality_rate) %>%
mutate(pm_nmx = pm_nmx/100000)
#>--------------------------------------------------------------------------------------------------------------------------->#
#> Calcualte "Actual" Global Life Table, for various PM2.5 levels (corresponding to start of Part-2 in the STATA script)--------------->#
# Use "adjusted" all cause mortality rates, to compute actual life expectancy at birth in a world where PM2.5 = x (following a combination of Apte's 2018 paper supplemental information doc and Arias 2013 paper, which follow same terminology).
# define alpha_x to be 0.5 for every age group (as in Apte et al.). alpha represents the fraction of the age interval duration that the average dying cohort member survives. Given this alpha_x = 0.5, tells us that we assume deaths take place at the midpoint of each age interval. Note that the dataset we use below is the same one that we prepped above, i.e. "cleaned_joined_mort_rr_data_long_summary", i.e. file 2 in the corresponding STATA script.
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long_summary %>%
mutate(alpha_x = 0.5)
# compute nqx (its the probability of death during the age interval of duration n years i.e.between age x and age x+n)
# nqx = ndx/lx (equation 3, Arias, et al, 2013 & also Eq.3 of Apte's 2018 supplemental information doc). ndx represents the number of life table cohort deaths in a given age interval (x, x+n), from a given cause.
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long_summary %>%
mutate(nqx = (nmx*age_gap)/(1 + ((1 - alpha_x)*(nmx)*(age_gap))))
# (* Assumption): set nqx = 1, if age_cat = max(age_cat), which as of now is "95+" and sorting by pm_level and age_interval
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long_summary %>%
mutate(nqx = ifelse(age_interval == "95+", 1, nqx)) %>%
arrange(pm_level, age_interval_ll)
#> Compute lx (Population of cohort still alive at the beginning of the age interval).
# lx is the survivor function. It considers the surviving population of a hypothetical birth cohort of 100000 individuals at age x. See equation 2 in Apte et al, 2018 (supplemental information doc). In other words, population of the lifetable cohort that is still alive at the beginning of age interval x depends on the number of individuals that are alive at the outset of the previous age interval x-l, and the fraction of members who survived that preceding age interval, (1 - nq(x-1)). For the first age interval (0-1), lx = 100000, because the everyone is alive at the beginning of the first age interval. For every interval, after the first one, we use the formula in equation 3 of the apte paper to calculate lx.
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long_summary %>%
mutate(lx = 100000) %>%
group_by(pm_level) %>%
mutate(temp_grp_wise_row_number = dplyr::row_number()) %>%  # adding this column so that we can avoid writing a loop to calculate lx in the next mutate statement
mutate(lx = ifelse(age_interval != "0-1", (lx[temp_grp_wise_row_number -1] * (1 - nqx[temp_grp_wise_row_number - 1])), 100000)) %>%
ungroup()
#> Compute ndx i.e. the life table cohort deaths in the given  age interval and nlx, i.e. life years lived by the cohort in the given age interval
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long_summary %>%
mutate(ndx = lx * nqx,
nlx = ndx/nmx)
#> Compute nfx (which is a preparation for calculating the global "cause deleted" life table, that will be calculated after we are done with the "actual life table" calculation.
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long_summary %>%
arrange(pm_level, age_interval_ll) %>%
mutate(nfx = 0) %>%
group_by(pm_level) %>%
mutate(nfx = (((age_gap[temp_grp_wise_row_number])*(lx[temp_grp_wise_row_number])) - nlx[temp_grp_wise_row_number])/(lx[temp_grp_wise_row_number] - lx[temp_grp_wise_row_number + 1])) %>%
ungroup()
#> compute ntx
# for age_interval_category == max(age_interval_category), set ntx = nlx
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long_summary %>%
mutate(ntx = NA,
ntx = ifelse(age_interval_category == max(age_interval_category), nlx, ntx))
# for the rest, for each pm_level (group by pm_level) -> update ntx, by starting from max(age_interval_category) - 1 (which in our case will be the 90-94 age interval) and then, go back till the first interval. This means that for a given pm_level group, we would start at the bottom of the group and then go up.
# number of unique age catgories
num_unique_age_cat <- max(cleaned_joined_mort_rr_data_long_summary$age_interval_category)
# number of unique pm levels (= nrow(cleaned_joined_mort_rr_data_long_summary)/num_unique_age_cat)
num_unique_pm_levels <- length(unique(cleaned_joined_mort_rr_data_long_summary$pm_level))
#
# for(i in 1:num_unique_pm_levels){
#   if(i == 1){
#      temp_grp <- cleaned_joined_mort_rr_data_long_summary[1 : num_unique_age_cat, ]
#   } else {
#     temp_grp <- cleaned_joined_mort_rr_data_long_summary[((((i - 1)*(num_unique_age_cat)) + 1) :(i * num_unique_age_cat)), ]
#   }
#    temp_grp <- temp_grp %>%
#       select(ntx, nlx)
#   print(str_c("Iteration number", i, sep = "-"))
#   for(j in (num_unique_age_cat - 1): 1){
#   temp_grp <- temp_grp %>%
#       mutate(ntx = nlx[j] + ntx[j + 1])
#   }
#   if(i == 1){
#      cleaned_joined_mort_rr_data_long_summary$ntx[1:num_unique_age_cat] <- temp_grp$ntx
#   } else {
#     cleaned_joined_mort_rr_data_long_summary$ntx[((((i - 1)*(num_unique_age_cat)) + 1) :(i * num_unique_age_cat))] <- temp_grp$ntx
#   }
#
# }
#
#
num_unique_age_cat
num_unique_pm_levels
View(cleaned_joined_mort_rr_data_long_summary)
((((2 - 1)*(num_unique_age_cat)) + 1) :(2 * num_unique_age_cat))
((((3 - 1)*(num_unique_age_cat)) + 1) :(3 * num_unique_age_cat))
((((18 - 1)*(num_unique_age_cat)) + 1) :(3 * num_unique_age_cat))
((((18 - 1)*(num_unique_age_cat)) + 1) :(18 * num_unique_age_cat))
((((2 - 1)*(num_unique_age_cat)) + 1) :(2 * num_unique_age_cat))
num_unique_age_cat
for(j in 20:1) print(j)
#
for(i in 1:num_unique_pm_levels){
if(i == 1){
temp_grp <- cleaned_joined_mort_rr_data_long_summary[1 : num_unique_age_cat, ]
} else {
temp_grp <- cleaned_joined_mort_rr_data_long_summary[((((i - 1)*(num_unique_age_cat)) + 1) :(i * num_unique_age_cat)), ]
}
temp_grp <- temp_grp %>%
select(ntx, nlx)
print(str_c("Iteration number", i, sep = "-"))
for(j in (num_unique_age_cat - 1): 1){
temp_grp$ntx[j] <- temp_grp$nlx[j] + temp_grp$ntx[j+1]
}
if(i == 1){
cleaned_joined_mort_rr_data_long_summary$ntx[1:num_unique_age_cat] <- temp_grp$ntx
} else {
cleaned_joined_mort_rr_data_long_summary$ntx[((((i - 1)*(num_unique_age_cat)) + 1) :(i * num_unique_age_cat))] <- temp_grp$ntx
}
}
plyr::round_any(73000, 0.000001)
plyr::round_any(73.00111, 0.000001)
plyr::round_any(0.733333, 0.000001)
plyr::round_any(72.112121212, 0.000001)
