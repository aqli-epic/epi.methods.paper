mutate(m_diff = adjusted_mortality_rate - mortality_rate)
# * (New Assumption): summarise "cleaned_joined_mort_rr_data_long" dataset by summing up "m_diff", grouped by age_cat, pm_level and then sort by "pm_level" and "age_cat". Note that this m_diff_sum (which is calculated using the adjusted mortality rates concerning the 6 causes of deaths) will be used to adjust all cause mortality rates later on in the process. It should be noted that the "m_diff_sum" that we will add to the "all cause mortality" later on in the process, is an adjustment based on the 6 disease channels in question.
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long %>%
group_by(age_interval_ll, pm_level) %>%
summarise(m_diff_sum = sum(m_diff, na.rm = TRUE)) %>%
ungroup() %>%
arrange(pm_level, age_interval_ll)
# (* New Assumption) merging the summary dataset generated in the above step with "cleaned_gbd_mortality_rates_all_causes" dataset, using "age_interval" as the linking key. Then rename the "mortality_rate" column to "actual_mr". Note that the "all_causes" dataset does not have a "pm_level" column. So, each age_category in the joined dataset, has an associated set of 21 pm_levels, but for all of those the mortality_rate remains the same (that is an assumption that we make). We then adjust these mortality rates, by adding in the "m_diff_sum" column to the "actual_mr" column (which is the new name of the "mortality_rate" column), the output of which is assigned to the column named "nMx".
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long_summary %>%
left_join(cleaned_gbd_mortality_rates_all_causes_only, by = "age_interval_ll") %>%
arrange(age_interval_ll, pm_level) %>%
rename(actual_mr = mortality_rate)
# add a new nMx column, which is created by adding the "m_diff_sum" adjustment to the all cause mortality rates. But, note that "m_diff_sum" was calculated from only 6 PM2.5 specific disease channels. This is what "all cause mortality" rates would be if PM2.5 = X (where "X" is a counterfactual concentration)
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long_summary %>%
mutate(nmx = actual_mr + m_diff_sum)
# replace nMx with nmx/100000 (this column represents the "Mortality rate (deaths per person-year), adjusted"): This dataset corresponds to the "file2" dataset in Ken's STATA script. nMx is the mortality incidence rate for age interval between ages x and x + n, expressed in the units of deaths per person-year
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long_summary %>%
mutate(nmx = nmx/100000)
#>----------------------------------------------------------------------------------------------------------->#
#> Prepare age-specific mortalities attributed to PM2.5 (for cause-deleted)------------------------------------------>#
# Note that "cleaned_joined_mort_rr_data_long" corresponds to the "file1" tempfile in the STATA scripts. Adding 2 new columns called paf (Population Attributable Fraction) and pm_mortal_rate
cleaned_joined_mort_rr_data_long <- cleaned_joined_mort_rr_data_long %>%
mutate(paf = 1 - (1/relative_risk),
pm_mortality_rate = paf * adjusted_mortality_rate)
# summary for cause-deleted life table (cdlt): This is the sum of total number of deaths across all 6 causes of death (corresponds to collapse statement that we see in part "f" of the STATA script).
cleaned_joined_mort_rr_data_long_summary_cdlt <- cleaned_joined_mort_rr_data_long %>%
group_by(age_interval, age_interval_ll, age_interval_ul, age_gap, pm_level) %>%
summarise(pm_mortality_rate = sum(pm_mortality_rate, na.rm = TRUE)) %>%
ungroup()
# rename pm_mortality_rate to pm_nmx and then converting pm_nmx into a rate by dividing by 100000. The resulting dataset corresponds to "file-3" in the STATA script
cleaned_joined_mort_rr_data_long_summary_cdlt <- cleaned_joined_mort_rr_data_long_summary_cdlt %>%
rename(pm_nmx = pm_mortality_rate) %>%
mutate(pm_nmx = pm_nmx/100000)
#>--------------------------------------------------------------------------------------------------------------------------->#
#> Calcualte "Actual" Global Life Table, for various PM2.5 levels (corresponding to start of Part-2 in the STATA script)--------------->#
# Use "adjusted" all cause mortality rates, to compute actual life expectancy at birth in a world where PM2.5 = x (following a combination of Apte's 2018 paper supplemental information doc and Arias 2013 paper, which follow same terminology).
# define alpha_x to be 0.5 for every age group (as in Apte et al.). alpha represents the fraction of the age interval duration that the average dying cohort member survives. Given this alpha_x = 0.5, tells us that we assume deaths take place at the midpoint of each age interval. Note that the dataset we use below is the same one that we prepped above, i.e. "cleaned_joined_mort_rr_data_long_summary", i.e. file 2 in the corresponding STATA script.
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long_summary %>%
mutate(alpha_x = 0.5)
# compute nqx (its the probability of death during the age interval of duration n years i.e.between age x and age x+n)
# nqx = ndx/lx (equation 3, Arias, et al, 2013 & also Eq.3 of Apte's 2018 supplemental information doc). ndx represents the number of life table cohort deaths in a given age interval (x, x+n), from a given cause.
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long_summary %>%
mutate(nqx = (nmx*age_gap)/(1 + ((1 - alpha_x)*(nmx)*(age_gap))))
# (* Assumption): set nqx = 1, if age_cat = max(age_cat), which as of now is "95+" and sorting by pm_level and age_interval
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long_summary %>%
mutate(nqx = ifelse(age_interval == "95+", 1, nqx)) %>%
arrange(pm_level, age_interval_ll)
#> Compute lx (Population of cohort still alive at the beginning of the age interval).
# lx is the survivor function. It considers the surviving population of a hypothetical birth cohort of 100000 individuals at age x. See equation 2 in Apte et al, 2018 (supplemental information doc). In other words, population of the lifetable cohort that is still alive at the beginning of age interval x depends on the number of individuals that are alive at the outset of the previous age interval x-l, and the fraction of members who survived that preceding age interval, (1 - nq(x-1)). For the first age interval (0-1), lx = 100000, because the everyone is alive at the beginning of the first age interval. For every interval, after the first one, we use the formula in equation 3 of the apte paper to calculate lx.
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long_summary %>%
mutate(lx = 100000) %>%
group_by(pm_level) %>%
mutate(temp_grp_wise_row_number = dplyr::row_number()) %>%  # adding this column so that we can avoid writing a loop to calculate lx in the next mutate statement
mutate(lx = ifelse(age_interval != "0-1", (lx[temp_grp_wise_row_number -1] * (1 - nqx[temp_grp_wise_row_number - 1])), 100000)) %>%
ungroup()
#> Compute ndx i.e. the life table cohort deaths in the given  age interval and nlx, i.e. life years lived by the cohort in the given age interval
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long_summary %>%
mutate(ndx = lx * nqx,
nlx = ndx/nmx)
#> Compute nfx (which is a preparation for calculating the global "cause deleted" life table, that will be calculated after we are done with the "actual life table" calculation.
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long_summary %>%
arrange(pm_level, age_interval_ll) %>%
mutate(nfx = 0) %>%
group_by(pm_level) %>%
mutate(nfx = (((age_gap[temp_grp_wise_row_number])*(lx[temp_grp_wise_row_number])) - nlx[temp_grp_wise_row_number])/(lx[temp_grp_wise_row_number] - lx[temp_grp_wise_row_number + 1])) %>%
ungroup()
#> compute ntx
# for age_interval_category == max(age_interval_category), set ntx = nlx
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long_summary %>%
mutate(ntx = NA,
ntx = ifelse(age_interval_category == max(age_interval_category), nlx, ntx))
# for the rest, for each pm_level (group by pm_level) -> update ntx, by starting from max(age_interval_category) - 1 (which in our case will be the 90-94 age interval) and then, go back till the first interval. This means that for a given pm_level group, we would start at the bottom of the group and then go up.
# number of unique age catgories
num_unique_age_cat <- max(cleaned_joined_mort_rr_data_long_summary$age_interval_category)
# number of unique pm levels (= nrow(cleaned_joined_mort_rr_data_long_summary)/num_unique_age_cat)
num_unique_pm_levels <- length(unique(cleaned_joined_mort_rr_data_long_summary$pm_level))
# Each pm level corresponds to 21 age categories and in total there are 18 unique pm levels. To calculate ntx, we use a nested for loop. The first for loop goes through each pm level and for each pm level we go through all age categories (except the max(age_interval_lower, i.e. 95+, for which we have already assigned a value above), but we start from the last age category - 1, i.e. the age category that is one below the max age category (for which we already have a value for ntx). Note that this happens for each pm level, which is why this process repeats 18 times as, 18 is the number of unique pm levels we have in our current dataset. All of this is done so that the following formula for ntx can be implemented in code: ntx = nlx[_n] + ntx[_n+1].
for(i in 1:num_unique_pm_levels){
if(i == 1){
temp_grp <- cleaned_joined_mort_rr_data_long_summary[1 : num_unique_age_cat, ]
} else {
temp_grp <- cleaned_joined_mort_rr_data_long_summary[((((i - 1)*(num_unique_age_cat)) + 1) :(i * num_unique_age_cat)), ]
}
temp_grp <- temp_grp %>%
select(ntx, nlx)
print(str_c("Iteration number", i, sep = "-"))
for(j in (num_unique_age_cat - 1): 1){
temp_grp$ntx[j] <- temp_grp$nlx[j] + temp_grp$ntx[j+1]
}
if(i == 1){
cleaned_joined_mort_rr_data_long_summary$ntx[1:num_unique_age_cat] <- temp_grp$ntx
} else {
cleaned_joined_mort_rr_data_long_summary$ntx[((((i - 1)*(num_unique_age_cat)) + 1) :(i * num_unique_age_cat))] <- temp_grp$ntx
}
}
#> Compute ex and then roud it to the nearest 1/1000000
cleaned_joined_mort_rr_data_long_summary <- cleaned_joined_mort_rr_data_long_summary %>%
mutate(ex = ntx/lx,
ex = plyr::round_any(ex, 0.000001))
#> Compute life expectancy at birth
actual_life_ex_birth <- cleaned_joined_mort_rr_data_long_summary %>%
filter(age_interval_ll == 0, pm_level == global_avg_pm2.5_rounded) %>%
summarise(avg_life_exp_at_birth = mean(ex))
#> At this stage create a copy of the cleaned_joined_mort_rr_data_long_summary and name it the "actual_table"
actual_life_table <- cleaned_joined_mort_rr_data_long_summary
#>------------------------------------------------------------------------------------------------------->#
#> ------------------Calculate "cause deleted" global life table, for various causes------------------------------------------->#
#>  We start of with the "cleaned_joined_mort_rr_data_long_summary_cdlt" data that we created above (one that corresponds to file-3 tempfile in the corresponding STATA script).
cleaned_joined_mort_rr_data_long_summary_cdlt
#> Part 1: Follow Apte et al and Arias et al procedure to derive the counterfactual probability of death after eliminating PM2.5 as a possible cause of death
# For this cause-deleted part of the script, we calculate similar variable as we did for the "actual_life_table" (which was generated using "all causes" data, which to be clear oes not mean sum of 6 disease channels, but rather the actual "all causes" category of data that we download from the GBD) part of the script, but assuming that the death by pm2.5 (occuring via the sum of the 6 disease channels) is eliminated. Given that assumption we calculate a new life table, which we name as the "cause eliminated" life table (because we have eliminated PM2.5 as a cause of death, by summing up the 6 causes mortality data, converted it into life expectancy numbers and then subtracted it from the actual life table). Given this, how does the life expectancy at different age intervals changes and how does that compare to the actual life table? This is the question that we finally answer by taking a difference between the actual table and the cause-eliminated table.
# The following computation is based on Apte et al, 2018 and Arias et al 2013 papers. The terminology is a mix of both so, refer both papers. Although Arias paper explains the underlying concepts in a much more clear fashion.
# Add a alpha_x column to the "cleaned_joined_mort_rr_data_long_summary_cdlt" dataset
cleaned_joined_mort_rr_data_long_summary_cdlt <- cleaned_joined_mort_rr_data_long_summary_cdlt %>%
mutate(alpha_x = 0.5)
# create a new "nqx_attrib" column (See Apte's paper supplemental information section): This is the age specific death rate, attributable to PM2.5
cleaned_joined_mort_rr_data_long_summary_cdlt <- cleaned_joined_mort_rr_data_long_summary_cdlt %>%
mutate(nqx_attrib = (pm_nmx * age_gap)/(1 + ((1 - alpha_x)*(pm_nmx)*(age_gap))))
# Join the above table with the "actual table" calculated in the section above.
cleaned_joined_mort_rr_data_long_summary_cdlt <- cleaned_joined_mort_rr_data_long_summary_cdlt %>%
left_join(actual_life_table, by = c("age_interval", "age_interval_ll", "age_interval_ul", "age_gap", "alpha_x", "pm_level"))
#  Compute nqx_deleted using nrx and nqx. nrx = nqx_attrib/nqx in Apte et al (2018) supplemental information section. This is equivalent to pm_nmx/nmx in Arias et al (Equation 6).
cleaned_joined_mort_rr_data_long_summary_cdlt <- cleaned_joined_mort_rr_data_long_summary_cdlt %>%
mutate(nrx = pm_nmx/nmx,
nqx_del = 1 - ((1 - nqx)^(1 - nrx)))
#> Part-2: Calculate cause deleted life expectancy
# Compute lx_del, which is the cause deleted population of cohort still alive at the beginning of age interval and sort by pm_level, age_interval_ll
cleaned_joined_mort_rr_data_long_summary_cdlt <- cleaned_joined_mort_rr_data_long_summary_cdlt %>%
mutate(lx_del = NA) %>%
arrange(pm_level, age_interval_ll)
# Compute "lx_del" (which is the cause-deleted version of lx, which we calculated in the actual life table)
cleaned_joined_mort_rr_data_long_summary_cdlt <- cleaned_joined_mort_rr_data_long_summary_cdlt %>%
mutate(lx_del = 100000) %>%
group_by(pm_level) %>%
mutate(lx_del = ifelse(age_interval != "0-1", (lx_del[temp_grp_wise_row_number - 1] * (1 - nqx_del[temp_grp_wise_row_number - 1])), 100000)) %>%
ungroup()
# Compute "nlx_del" (which is the corresponding version of nlx, which we calculated in the actual life table) using nfx from "actual life table"
cleaned_joined_mort_rr_data_long_summary_cdlt <- cleaned_joined_mort_rr_data_long_summary_cdlt %>%
mutate(nlx_del = NA) %>%
group_by(pm_level) %>%
mutate(nlx_del = ((age_gap[temp_grp_wise_row_number] - nfx[temp_grp_wise_row_number]) * (lx_del[temp_grp_wise_row_number])) + (nfx[temp_grp_wise_row_number] * lx[temp_grp_wise_row_number + 1])) %>%
ungroup()
#> Compute tx_del
cleaned_joined_mort_rr_data_long_summary_cdlt <- cleaned_joined_mort_rr_data_long_summary_cdlt %>%
mutate(tx_del = NA,
temp_row_num = row_number(),
tx_del = ifelse(age_interval == "95+", (ex[temp_row_num]*lx_del[temp_row_num])/(1 - nrx[temp_row_num]), NA))
# Each pm level corresponds to 21 age categories and in total there are 18 unique pm levels. To calculate tx_del, we use a nested for loop. The first for loop goes through each pm level and for each pm level we go through all age categories (except the max(age_interval_lower, i.e. 95+, for which we have already assigned a value above), but we start from the "last age category - 1", i.e. the age category that is one below the max age category (for which we already have a value for tx_del). Note that this happens for each pm level, which is why this process repeats 18 times as, 18 is the number of unique pm levels we have in our current dataset. All of this is done so that the following formula for tx_del can be implemented in code: tx_del[current_grp_wise_row_number + 1] + nlx_del[current_grp_wise_row_number]
for(i in 1:num_unique_pm_levels){
if(i == 1){
temp_grp <- cleaned_joined_mort_rr_data_long_summary_cdlt[1 : num_unique_age_cat, ]
} else {
temp_grp <- cleaned_joined_mort_rr_data_long_summary_cdlt[((((i - 1)*(num_unique_age_cat)) + 1) : (i * num_unique_age_cat)), ]
}
temp_grp <- temp_grp %>%
select(ex, lx_del, nrx, tx_del, nlx_del) %>%
print(str_c("Iteration number", i, sep = "-"))
for(j in (num_unique_age_cat - 1): 1){
temp_grp$tx_del[j] <- temp_grp$nlx_del[j] + temp_grp$tx_del[j+1]
}
if(i == 1){
cleaned_joined_mort_rr_data_long_summary_cdlt$tx_del[1:num_unique_age_cat] <- temp_grp$tx_del
} else {
cleaned_joined_mort_rr_data_long_summary_cdlt$tx_del[((((i - 1)*(num_unique_age_cat)) + 1) :(i * num_unique_age_cat))] <- temp_grp$tx_del
}
}
#> Compute counterfactual life expectancy at any given age interval "x"
cleaned_joined_mort_rr_data_long_summary_cdlt <- cleaned_joined_mort_rr_data_long_summary_cdlt %>%
mutate(counterfactual_life_exp = tx_del/lx_del,
counterfactual_life_exp = plyr::round_any(counterfactual_life_exp, 0.000001))
#> create final dataset (which is a subset of "cleaned_joined_mort_rr_data_long_summary_cdlt") for plotting and generate a "life_years_lost" column, which represents life years lost due to PM2.5 risk. Also generate a new "row number" column in this final dataset.
cleaned_joined_mort_rr_data_long_summary_cdlt_subset <- cleaned_joined_mort_rr_data_long_summary_cdlt %>%
filter(age_interval == "0-1") %>%
rename(ex_actual_life_table = ex) %>%
mutate(life_years_lost = counterfactual_life_exp - ex_actual_life_table,
row_id = row_number()) %>%
select(pm_level, ex_actual_life_table, counterfactual_life_exp, life_years_lost, row_id) %>%
rename(life_years_lost_ier2019 = life_years_lost) %>%
arrange(pm_level) %>%
arrange(row_id) %>%
select(row_id, pm_level, everything())
#> sanity check: life_years_lost > 0 ?
#> final methods analysis dataset (adding a life years lost column (based on the AQLI methodology) to the  "cleaned_joined_mort_rr_data_long_summary_cdlt_subset" dataset)
final_methods_analysis_dataset <- cleaned_joined_mort_rr_data_long_summary_cdlt_subset %>%
mutate(life_years_lost_aqli = (pm_level - who_pm2.5_guideline)*aqli_lyl_constant,
life_years_lost_aqli = ifelse(life_years_lost_aqli < 0, 0, life_years_lost_aqli))
#> Plotting Life Expectancy Loss as a function of PM2.5 concentration
foo <- final_methods_analysis_dataset %>%
filter(pm_level <= 120) %>%
ggplot() +
geom_point(mapping = aes(x = pm_level, y = life_years_lost_ier2019), color = "red") +
geom_smooth(mapping = aes(x = pm_level, y = life_years_lost_ier2019), color = "red", size = 1.3, se = FALSE, linetype = "solid") +
geom_point(mapping = aes(x = pm_level, y = life_years_lost_aqli), color = "blue") +
geom_smooth(mapping = aes(x = pm_level, y = life_years_lost_aqli), color = "blue", size = 1.3,  se = FALSE) +
scale_y_continuous(breaks = seq(0, 11, 1)) +
scale_x_continuous(breaks = seq(0, 130, 10)) +
theme_minimal()
#> Approximating a derivative function using the "splinefun" function
ier_gbd_2019_empirical <- splinefun(x = final_methods_analysis_dataset$pm_level,
y = final_methods_analysis_dataset$life_years_lost_ier2019)
# aqli empirical derivative fun
aqli_empirical <- splinefun(x = final_methods_analysis_dataset$pm_level,
y = final_methods_analysis_dataset$life_years_lost_aqli)
ier_gbd_2019_empirical(10:120)
foo <- final_methods_analysis_dataset %>%
filter(pm_level <= 120) %>%
ggplot() +
geom_point(mapping = aes(x = pm_level, y = life_years_lost_ier2019), color = "red") +
geom_smooth(mapping = aes(x = pm_level, y = life_years_lost_ier2019), color = "red", size = 1.3, se = FALSE, linetype = "solid") +
geom_point(mapping = aes(x = pm_level, y = life_years_lost_aqli), color = "blue") +
geom_smooth(mapping = aes(x = pm_level, y = life_years_lost_aqli), color = "blue", size = 1.3,  se = FALSE) +
scale_y_continuous(breaks = seq(0, 11, 1)) +
scale_x_continuous(breaks = seq(0, 130, 10)) +
theme_minimal()
foo
# plotting empirical version of plot 1
ggplot(data = tibble(x = 0:10), aes(x = x)) +
geom_function(fun = ier_gbd_2019_empirical())
# plotting empirical version of plot 1
ggplot(data = tibble(x = 0:10), mapping = aes(x = x)) +
geom_function(fun = ier_gbd_2019_empirical())
# plotting empirical version of plot 1
ggplot(data = tibble(x = 0:10), mapping = aes(x = x)) +
geom_function(fun = ier_gbd_2019_empirical)
# plotting empirical version of plot 1
ggplot(data = tibble(x = 0:120), mapping = aes(x = x)) +
geom_function(fun = ier_gbd_2019_empirical)
foo
# plotting empirical version of plot 1
ggplot(data = tibble(x = 0:120), mapping = aes(x = x)) +
geom_function(fun = ier_gbd_2019_empirical) +
geom_function(fun = aqli_empirical)
# plotting empirical version of plot 1
ggplot(data = tibble(x = 0:120), mapping = aes(x = x)) +
geom_function(fun = ier_gbd_2019_empirical) +
geom_function(fun = aqli_empirical)
foo
# plotting empirical version of plot 1
ggplot(data = tibble(x = 0:120), mapping = aes(x = x)) +
geom_function(fun = ier_gbd_2019_empirical) +
geom_function(fun = aqli_empirical) +
scale_y_continuous(breaks = seq(0, 10, 1))
# plotting empirical version of plot 1
ggplot(data = tibble(x = 0:120), mapping = aes(x = x)) +
geom_function(fun = ier_gbd_2019_empirical) +
geom_function(fun = aqli_empirical) +
scale_y_continuous(breaks = seq(0, 10, 1))
foo
# plotting empirical version of plot 1
ggplot(data = tibble(x = 0:120), mapping = aes(x = x)) +
geom_function(fun = ier_gbd_2019_empirical, size = 1, args = list(deriv = 1)) +
geom_function(fun = aqli_empirical) +
scale_y_continuous(breaks = seq(0, 10, 1), size = 1, args = list(deriv = 1))
# plotting empirical version of plot 1
ggplot(data = tibble(x = 0:120), mapping = aes(x = x)) +
geom_function(fun = ier_gbd_2019_empirical, size = 1, args = list(deriv = 1)) +
geom_function(fun = aqli_empirical, size = 1, args = list(deriv = 1)) +
scale_y_continuous(breaks = seq(0, 10, 1))
# plotting empirical version of plot 1
ggplot(data = tibble(x = 0:120), mapping = aes(x = x)) +
geom_function(fun = ier_gbd_2019_empirical, size = 1, args = list(deriv = 1)) +
geom_function(fun = aqli_empirical, size = 1, args = list(deriv = 1)) +
scale_y_continuous(breaks = seq(0, 10, 1)) +
scale_x_continuous(breaks = seq(0, 130, 10))
# plotting empirical version of plot 1
ggplot(data = tibble(x = 0:120), mapping = aes(x = x)) +
geom_function(fun = ier_gbd_2019_empirical, size = 1, args = list(deriv = 1)) +
geom_function(fun = aqli_empirical, size = 1, args = list(deriv = 1)) +
scale_x_continuous(breaks = seq(0, 130, 10))
# plotting empirical version of plot 1
ggplot(data = tibble(x = 0:120), mapping = aes(x = x)) +
geom_function(fun = ier_gbd_2019_empirical, size = 1, args = list(deriv = 1)) +
geom_function(fun = aqli_empirical, size = 1, args = list(deriv = 1)) +
scale_y_continuous(breaks = seq(0, 0.2, 0.025)) +
scale_x_continuous(breaks = seq(0, 130, 10))
# plotting empirical version of plot 1
ggplot(data = tibble(x = 1:120), mapping = aes(x = x)) +
geom_function(fun = ier_gbd_2019_empirical, size = 1, args = list(deriv = 1)) +
geom_function(fun = aqli_empirical, size = 1, args = list(deriv = 1)) +
scale_y_continuous(breaks = seq(0, 0.2, 0.025)) +
scale_x_continuous(breaks = seq(0, 130, 10))
# plotting empirical version of plot 1
ggplot(data = tibble(x = 1:120), mapping = aes(x = x)) +
geom_function(fun = ier_gbd_2019_empirical, size = 1, args = list(deriv = 1), color = "red") +
geom_function(fun = aqli_empirical, size = 1, args = list(deriv = 1)) +
scale_y_continuous(breaks = seq(0, 0.2, 0.025)) +
scale_x_continuous(breaks = seq(0, 130, 10))
# plotting empirical version of plot 1
ggplot(data = tibble(x = 1:120), mapping = aes(x = x)) +
geom_function(fun = ier_gbd_2019_empirical, size = 1, args = list(deriv = 1), color = "red") +
geom_function(fun = aqli_empirical, size = 1, args = list(deriv = 1), color = "blue") +
scale_y_continuous(breaks = seq(0, 0.2, 0.025)) +
scale_x_continuous(breaks = seq(0, 130, 10))
# plotting empirical version of plot 1
ggplot(data = tibble(x = 1:120), mapping = aes(x = x)) +
geom_function(fun = ier_gbd_2019_empirical, size = 1, args = list(deriv = 1), color = "red") +
geom_function(fun = aqli_empirical, size = 1, args = list(deriv = 1), color = "blue") +
scale_y_continuous(breaks = seq(0, 0.2, 0.025)) +
scale_x_continuous(breaks = seq(0, 130, 10)) +
ggthemes::theme_clean()
#> final methods analysis dataset (adding a life years lost column (based on the AQLI methodology) to the  "cleaned_joined_mort_rr_data_long_summary_cdlt_subset" dataset)
final_methods_analysis_dataset <- cleaned_joined_mort_rr_data_long_summary_cdlt_subset %>%
mutate(life_years_lost_aqli = (pm_level - who_pm2.5_guideline)*aqli_lyl_constant)
foo <- final_methods_analysis_dataset %>%
filter(pm_level <= 120) %>%
ggplot() +
geom_point(mapping = aes(x = pm_level, y = life_years_lost_ier2019), color = "red") +
geom_smooth(mapping = aes(x = pm_level, y = life_years_lost_ier2019), color = "red", size = 1.3, se = FALSE, linetype = "solid") +
geom_point(mapping = aes(x = pm_level, y = life_years_lost_aqli), color = "blue") +
geom_smooth(mapping = aes(x = pm_level, y = life_years_lost_aqli), color = "blue", size = 1.3,  se = FALSE) +
scale_y_continuous(breaks = seq(0, 11, 1)) +
scale_x_continuous(breaks = seq(0, 130, 10)) +
theme_minimal()
#> Approximating a derivative function using the "splinefun" function
ier_gbd_2019_empirical <- splinefun(x = final_methods_analysis_dataset$pm_level,
y = final_methods_analysis_dataset$life_years_lost_ier2019)
# aqli empirical derivative fun
aqli_empirical <- splinefun(x = final_methods_analysis_dataset$pm_level,
y = final_methods_analysis_dataset$life_years_lost_aqli)
# plotting empirical version of plot 1
ggplot(data = tibble(x = 1:120), mapping = aes(x = x)) +
geom_function(fun = ier_gbd_2019_empirical, size = 1, args = list(deriv = 1), color = "red") +
geom_function(fun = aqli_empirical, size = 1, args = list(deriv = 1), color = "blue") +
scale_y_continuous(breaks = seq(0, 0.2, 0.025)) +
scale_x_continuous(breaks = seq(0, 130, 10)) +
ggthemes::theme_clean()
?pivot_longer
#> create a long version of the final analysis dataset
final_methods_analysis_dataset_long <- final_methods_analysis_dataset %>%
pivot_longer(cols = life_years_lost_ier2019:life_years_lost_aqli, names_to = "method_name",
values_to = life_years_lost)
#> create a long version of the final analysis dataset
final_methods_analysis_dataset_long <- final_methods_analysis_dataset %>%
pivot_longer(cols = life_years_lost_ier2019:life_years_lost_aqli, names_to = "method_name",
values_to = "life_years_lost")
final_methods_analysis_dataset_long
View(final_methods_analysis_dataset_long)
#> Plot 1 (using long): Plotting Life Expectancy loss as a function of PM2.5 concentration
final_methods_analysis_dataset_long %>%
ggplot(mapping = aes(x = pm_level, y = life_years_lost)) +
geom_point(mapping = aes(color = method_name)) +
geom_line(mapping = aes(color = mehtod_name))
#> Plot 1 (using long): Plotting Life Expectancy loss as a function of PM2.5 concentration
final_methods_analysis_dataset_long %>%
ggplot(mapping = aes(x = pm_level, y = life_years_lost)) +
geom_point(mapping = aes(color = method_name)) +
geom_line(mapping = aes(color = method_name))
splinefun(x= 1:10)
ier_gbd_2019_empirical(1:10)
# plotting empirical version of plot 1
ggplot(data = tibble(x = 1:120), mapping = aes(x = x)) +
geom_function(fun = ier_gbd_2019_empirical, size = 1, args = list(deriv = 1), color = "red") +
geom_function(fun = aqli_empirical, size = 1, args = list(deriv = 1), color = "blue") +
scale_y_continuous(breaks = seq(0, 0.2, 0.025)) +
scale_x_continuous(breaks = seq(0, 130, 10)) +
ggthemes::theme_clean() +
labs(x = "PM level", y = "Life Years Lost/microgram per cubic meter")
# plotting empirical version of plot 1
ggplot(data = tibble(x = 1:120), mapping = aes(x = x)) +
geom_function(fun = ier_gbd_2019_empirical, size = 1, args = list(deriv = 1), color = "red") +
geom_function(fun = aqli_empirical, size = 1, args = list(deriv = 1), color = "blue") +
scale_y_continuous(breaks = seq(0, 0.2, 0.025)) +
scale_x_continuous(breaks = seq(0, 130, 10)) +
ggthemes::theme_clean() +
labs(x = "PM level", y = "Life Years Lost/microgram per cubic meter") +
scale_color_manual(c("red" = "GBD 2019 (IER)", "blue" = "AQLI"))
# plotting empirical version of plot 1
ggplot(data = tibble(x = 1:120), mapping = aes(x = x)) +
geom_function(fun = ier_gbd_2019_empirical, size = 1, args = list(deriv = 1), color = "red") +
geom_function(fun = aqli_empirical, size = 1, args = list(deriv = 1), color = "blue") +
scale_y_continuous(breaks = seq(0, 0.2, 0.025)) +
scale_x_continuous(breaks = seq(0, 130, 10)) +
ggthemes::theme_clean() +
labs(x = "PM level (microgram per cubic meter)", y = "Life Years Lost/microgram per cubic meter") +
scale_color_manual(c("red" = "GBD 2019 (IER)", "blue" = "AQLI"))
# plotting empirical version of plot 1
ggplot(data = tibble(x = 1:120), mapping = aes(x = x)) +
geom_function(fun = ier_gbd_2019_empirical, size = 1, args = list(deriv = 1), color = "red") +
geom_function(fun = aqli_empirical, size = 1, args = list(deriv = 1), color = "blue") +
scale_y_continuous(breaks = seq(0, 0.2, 0.025), limits = c(0, .175)) +
scale_x_continuous(breaks = seq(0, 130, 10)) +
ggthemes::theme_hc() +
labs(x = "PM level (microgram per cubic meter)", y = "Life Years Lost/microgram per cubic meter") +
scale_color_manual(values = c("GBD(IER 2019)" = "red", "AQLI" = "blue"))
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)
# libraries
library(readr)
library(dplyr)
library(stringr)
library(magrittr)
library(ggplot2)
library(readr)
library(tidytext)
library(tidyr)
library(tidyverse)
library(sf)
library(usethis)
library(devtools)
library(readxl)
# load and document (for automatic rendering of the roxygen comments into a man file) to keep things updated
devtools::load_all()
devtools::document()
# custom operators
`%notin%` <- Negate(`%in%`)
# global parameters
gbd_year <- 2019
#> reading in the raw .xlsx file for relative risks dataset from GBD for the year mentioned up top in the "lib_glob_parameters" code chunk
rr_dataset_2019_raw <- readxl::read_xlsx("./data/raw/ihme_gbd_2019_relative_risks_by_cause.xlsx")
#> creating a copy of the raw dataset and then using that for all analysis
rr_dataset_2019 <- rr_dataset_2019_raw
#> adding some basic structure to the raw data file such that we can clearly recognize the column names before we start cleaning. In doing so, remove the unnecessary descritive information from the file and only keep the information that is relevant for analysis.
# assign column names, which as of now are present in different rows (because the raw file was formatted this way).
colnames(rr_dataset_2019)[1:4] <- rr_dataset_2019[1, 1:4]
colnames(rr_dataset_2019)[5:ncol(rr_dataset_2019)] <- rr_dataset_2019[2, 5:ncol(rr_dataset_2019)]
# getting rid of first and second row of the dataset because they contain "column name" information, which has already been placed in its proper place in the step above.
rr_dataset_2019 <- rr_dataset_2019[4:nrow(rr_dataset_2019), ]
# rename certain columns (just removing spaces in between and converting to snake case)
colnames(rr_dataset_2019)[1] <- "cause" # "Risk/Outcome" column renamed to "Cause"
colnames(rr_dataset_2019)[2] <- "pm_level"
colnames(rr_dataset_2019)[3] <- "mortality_morbidity"
colnames(rr_dataset_2019)[4] <- "sex"
colnames(rr_dataset_2019)[5] <- "all_ages"
# removing the word "years" from the "ages" column names and also remove  any "+" signs in age columns (e.g. "95+")
age_name_column_names_vec <- colnames(rr_dataset_2019)[str_detect(colnames(rr_dataset_2019), "years")]
age_name_column_names_vec_years_removed <- str_replace(age_name_column_names_vec, "years", "")
# age_name_column_names_vec_years_removed <- str_replace(age_name_column_names_vec_years_removed, "\\+", "")
# rename "ages" column using the new "years removed age column names"
colnames(rr_dataset_2019)[str_detect(colnames(rr_dataset_2019), "years")] <- age_name_column_names_vec_years_removed
# remove the "µg/m³" sign from the pm_level column
rr_dataset_2019[, "pm_level"] <- str_remove(as.vector(unlist(rr_dataset_2019[, "pm_level"])), "µg/m³")
# keep only the relative risks numbers and remove all confidence interval information from all columns. For example: if we have this: "2.345 (1.234 to 3.232)", after cleaning we should be left with "2.345".
rr_dataset_2019[, 5:ncol(rr_dataset_2019)] <- map_dfc(rr_dataset_2019[, 5:ncol(rr_dataset_2019)], function(x){as.numeric(str_replace(x, "\\(.+\\)$", ""))})
# dropping certain unnecessary identifier columns: "mortality_morbidity" (whose value = 'both' for all rows), "sex" (whose value = 'both' for all rows)
rr_dataset_2019 <- rr_dataset_2019 %>%
select(-c(mortality_morbidity, sex))
#> impute relative risk values for all age categories of the following causes: "Lower Respiratory Infections", "Tracheal, Bronchus and lung cancer", "Chronic Obstructive Pulmonary Disease", "Diabetes mellitus type 2" by using the "all_ages" column relative risks. The assumption is that relative risks for these diseases do not vary by age, so the imputation simply copies the "all_ages" column number for a given cause "x" (the ones listed in this paragraph) into all other age categories for cause "x". Need to check if this assumption is actually backed by solid research.
#> Note that it is possible that in the coming years, age wise relative risks data starts to become available for the above 4 causes. This is why the vector named "age_wise_rr_data_not_available" below may change (and has to be accordingly updated) from one year to the next. Please make sure to do so before running the code.
age_wise_rr_data_available <- c("Ischaemic heart disease", "Stroke")
# list of all "unique" causes in the relative risks data
all_causes_list <- unique(rr_dataset_2019$cause)
# Please double check this for the latest dataset in question. This specifically refers to the number of the column from where the age categories start. In the current dataset, the first age category (i.e. 25-29) is the fourth column, which is why as of now it equates to 4.
age_categories_col_start_number <- 4
# This loop goes through the relative risks dataset row by row and if a cause is NOT in "age_wise_rr_data_available" list of causes, it copies the "all_ages" value into all other age category columns. This is following from the imputation assumption explained above.
for(i in 1:nrow(rr_dataset_2019)){
if(rr_dataset_2019$cause[i] %in% age_wise_rr_data_available){
next
} else {
rr_dataset_2019[i, age_categories_col_start_number:ncol(rr_dataset_2019)] <- as.numeric(rr_dataset_2019[i, "all_ages"])
}
}
#> now we don't need the "all_ages" column so I am dropping it
rr_dataset_2019 <- rr_dataset_2019 %>%
select(-c(all_ages))
#> reshaping the above dataset to a long format, so that age category becomes a single column. Note: there is an additional space in each of the age cateogry column names. For example: the column name "25-29" is actually "25-29 ".
rr_dataset_2019_long <- rr_dataset_2019 %>%
pivot_longer(cols = `25-29 `:`95+ `, names_to = "age_interval", values_to = "relative_risk")
#> add an age_interval_lower column to the relative risks dataset
rr_dataset_2019_long <- rr_dataset_2019_long %>%
mutate(age_interval_ll = str_extract(age_interval, "(.+)-"),
age_interval_ll = str_extract(age_interval_ll, "[^-]+"),
age_interval_ul = str_extract(age_interval, "-(.+)"),
age_interval_ul = str_extract(age_interval_ul, "[^-]+"),
age_interval_ll = ifelse(str_detect(age_interval, "95"), 95, age_interval_ll),
age_interval_ul = ifelse(str_detect(age_interval, "95"), 95, age_interval_ul))
#> encode the "cause" column for consistency, i.e. create a new column called "cause_id" that will map
#> each "cause_name" to a corresponding identifier number. Use the same encoding in the mortality rates cleaning file.
rr_dataset_2019_long <- rr_dataset_2019_long %>%
mutate(cause_id = ifelse(cause == "Lower respiratory infections", 322, cause),
cause_id = ifelse(cause == "Tracheal, bronchus, and lung cancer", 426,  cause_id),
cause_id = ifelse(cause == "Ischemic heart disease", 493, cause_id),
cause_id = ifelse(cause == "Stroke", 494, cause_id),
cause_id = ifelse(cause == "Chronic obstructive pulmonary disease", 509, cause_id),
cause_id = ifelse(cause == "Diabetes mellitus type 2", 587, cause_id))
#> sorting the dataset by cause, age_interval, pm_level columns
rr_dataset_2019_long <- rr_dataset_2019_long %>%
arrange(cause, age_interval, pm_level)
#> coercing certain numeric columns to class "numeric"
rr_dataset_2019_long$pm_level <- as.numeric(rr_dataset_2019_long$pm_level)
rr_dataset_2019_long$relative_risk <- as.numeric(rr_dataset_2019_long$relative_risk)
rr_dataset_2019_long$age_interval_ll <- as.numeric(rr_dataset_2019_long$age_interval_ll)
rr_dataset_2019_long$age_interval_ul <- as.numeric(rr_dataset_2019_long$age_interval_ul)
rr_dataset_2019_long$cause_id <- as.numeric(rr_dataset_2019_long$cause_id)
#> reshaping the relative risks data back to wide, moving the pm level wise relative risks in the columns, so that the [cause, age_interval] combination can become a unique identifier. This way, we can easily merge the relative risks dataset with the mortality rates dataset.
rr_dataset_2019_wide_final <- rr_dataset_2019_long %>%
pivot_wider(names_from = "pm_level", names_prefix = "relative_risk_pm", values_from = "relative_risk")
#> writing the cleaned relative risks file to the data/intermediate folder
rr_dataset_2019_wide_final %>%
write_csv("./data/intermediate/cleaned_relative_risks.csv")
# next step is to merge this dataset with the cleaned mortality rates dataset, which will happen in the "calc_mortality_rates.Rmd" file
rr_dataset_2019_wide_final
View(rr_dataset_2019_raw)
0.006 + 0.071*(1/3) + 0.003 + 0.0002
0.006 + 0.071*(1/3) + 0.003 + 0.0002 + 1
0.006 + 0.071*(1/3) + 1
0.006 + 0.071*(1/120) + 0.003 + 0.0002 + 1
0.006 + 0.071*(120) + 0.003 + 0.0002 + 1
0.006 + 0.071*(120) + 0.003 + 0.0002
0.006 + 0.071*(1/120) + 0.003 + 0.0002
0.006 + 0.071*(1/120) + 0.003 + 0.0002 + 1
0.006 + (1/120) + 0.003 + 0.0002 + 1
0.006 + (1/3) + 0.003 + 0.0002 + 1
0.006 + 0.071 + (1/3) + 0.003 + 0.0002 + 1
0.071 + (1/3) + 0.003 + 0.0002 + 1
install.packages("metafor")
library(metafor)
help(metafor)
?rma.mv
dat <- escalc(measure="OR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)
rma.mv(yi, vi, random = ~ 1 | trial, data=dat)
0.006 + 0.071*(1/3)
0.006 + 0.071*(1/120)
